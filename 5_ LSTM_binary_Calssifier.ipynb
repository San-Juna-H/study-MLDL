{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Introduction\n","\n","based on : https://github.com/bentrevett/pytorch-sentiment-analysis\n","\n","one-layer RNN Classifier with IMDB datasets"]},{"cell_type":"markdown","metadata":{},"source":["# 0. Set Environment"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T05:51:20.087127Z","iopub.status.busy":"2024-05-09T05:51:20.086594Z","iopub.status.idle":"2024-05-09T05:51:20.094575Z","shell.execute_reply":"2024-05-09T05:51:20.092979Z","shell.execute_reply.started":"2024-05-09T05:51:20.087090Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n","import torchtext\n","torchtext.disable_torchtext_deprecation_warning()\n","from torchtext.data import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","from torch.utils.tensorboard import SummaryWriter\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","from pprint import pprint\n","\n","import subprocess\n","import os\n","import sys\n","\n","import datasets"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T04:33:02.055460Z","iopub.status.busy":"2024-05-09T04:33:02.055071Z","iopub.status.idle":"2024-05-09T04:33:02.061661Z","shell.execute_reply":"2024-05-09T04:33:02.060618Z","shell.execute_reply.started":"2024-05-09T04:33:02.055426Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Selected device: cuda\n"]}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(\"Selected device:\", device)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T04:33:02.063226Z","iopub.status.busy":"2024-05-09T04:33:02.062835Z","iopub.status.idle":"2024-05-09T04:33:02.076477Z","shell.execute_reply":"2024-05-09T04:33:02.075456Z","shell.execute_reply.started":"2024-05-09T04:33:02.063197Z"},"trusted":true},"outputs":[],"source":["seed = 42\n","\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.backends.cudnn.deterministic = True"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["model_dir = './models/LSTM_Classifier_model.pth'\n","pretrained_embedding_dir = './models/Glove_pretrained.pth'"]},{"cell_type":"markdown","metadata":{},"source":["# 1. Data processing"]},{"cell_type":"markdown","metadata":{},"source":["## 1-1. Get Data"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T04:33:02.087466Z","iopub.status.busy":"2024-05-09T04:33:02.087111Z","iopub.status.idle":"2024-05-09T04:33:08.436868Z","shell.execute_reply":"2024-05-09T04:33:08.435884Z","shell.execute_reply.started":"2024-05-09T04:33:02.087440Z"},"trusted":true},"outputs":[],"source":["train_data, test_data = datasets.load_dataset(\"imdb\", split=[\"train\", \"test\"])"]},{"cell_type":"markdown","metadata":{},"source":["## 1-2. Tokenize"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T04:33:12.928637Z","iopub.status.busy":"2024-05-09T04:33:12.928257Z","iopub.status.idle":"2024-05-09T04:33:12.934105Z","shell.execute_reply":"2024-05-09T04:33:12.933039Z","shell.execute_reply.started":"2024-05-09T04:33:12.928609Z"},"trusted":true},"outputs":[],"source":["tokenizer = get_tokenizer(\"basic_english\")"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T04:33:17.496593Z","iopub.status.busy":"2024-05-09T04:33:17.495530Z","iopub.status.idle":"2024-05-09T04:33:17.500898Z","shell.execute_reply":"2024-05-09T04:33:17.500118Z","shell.execute_reply.started":"2024-05-09T04:33:17.496548Z"},"trusted":true},"outputs":[],"source":["def tokenize_example(example, tokenizer, max_length):\n","    tokens = tokenizer(example[\"text\"])[:max_length]\n","    length = len(tokens)\n","    return {\"tokens\": tokens, \"length\": length}"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T04:33:18.079017Z","iopub.status.busy":"2024-05-09T04:33:18.076194Z","iopub.status.idle":"2024-05-09T04:33:18.092531Z","shell.execute_reply":"2024-05-09T04:33:18.091640Z","shell.execute_reply.started":"2024-05-09T04:33:18.078945Z"},"trusted":true},"outputs":[],"source":["max_length = 256\n","\n","train_data = train_data.map(\n","    tokenize_example, fn_kwargs={\"tokenizer\": tokenizer, \"max_length\": max_length}\n",")\n","test_data = test_data.map(\n","    tokenize_example, fn_kwargs={\"tokenizer\": tokenizer, \"max_length\": max_length}\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## 1-3. Build Vocab "]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T04:33:50.933785Z","iopub.status.busy":"2024-05-09T04:33:50.933393Z","iopub.status.idle":"2024-05-09T04:33:57.179373Z","shell.execute_reply":"2024-05-09T04:33:57.178242Z","shell.execute_reply.started":"2024-05-09T04:33:50.933754Z"},"trusted":true},"outputs":[],"source":["min_freq = 5\n","special_tokens = [\"<unk>\", \"<pad>\"]\n","\n","vocab = build_vocab_from_iterator(train_data['tokens'],\n","                                  min_freq = min_freq,\n","                                  specials = special_tokens)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T04:34:23.087226Z","iopub.status.busy":"2024-05-09T04:34:23.086817Z","iopub.status.idle":"2024-05-09T04:34:23.092281Z","shell.execute_reply":"2024-05-09T04:34:23.091119Z","shell.execute_reply.started":"2024-05-09T04:34:23.087194Z"},"trusted":true},"outputs":[],"source":["unk_index = vocab[\"<unk>\"]\n","pad_index = vocab[\"<pad>\"]\n","\n","vocab.set_default_index(unk_index)"]},{"cell_type":"markdown","metadata":{},"source":["## 1-4. Numericalize Text"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T04:41:08.959882Z","iopub.status.busy":"2024-05-09T04:41:08.958858Z","iopub.status.idle":"2024-05-09T04:41:08.964755Z","shell.execute_reply":"2024-05-09T04:41:08.963746Z","shell.execute_reply.started":"2024-05-09T04:41:08.959844Z"},"trusted":true},"outputs":[],"source":["def numericalize_example(example, vocab):\n","    ids = vocab.lookup_indices(example[\"tokens\"])\n","    return {\"ids\": ids}"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T04:41:09.115654Z","iopub.status.busy":"2024-05-09T04:41:09.115293Z","iopub.status.idle":"2024-05-09T04:42:03.364867Z","shell.execute_reply":"2024-05-09T04:42:03.363650Z","shell.execute_reply.started":"2024-05-09T04:41:09.115628Z"},"trusted":true},"outputs":[],"source":["train_data = train_data.map(numericalize_example, fn_kwargs={\"vocab\": vocab})\n","test_data = test_data.map(numericalize_example, fn_kwargs={\"vocab\": vocab})"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T04:44:34.503457Z","iopub.status.busy":"2024-05-09T04:44:34.502345Z","iopub.status.idle":"2024-05-09T04:44:34.511254Z","shell.execute_reply":"2024-05-09T04:44:34.510035Z","shell.execute_reply.started":"2024-05-09T04:44:34.503402Z"},"trusted":true},"outputs":[],"source":["train_data = train_data.with_format(type=\"torch\", columns=[\"ids\", \"label\", \"length\"])\n","test_data = test_data.with_format(type=\"torch\", columns=[\"ids\", \"label\", \"length\"])"]},{"cell_type":"markdown","metadata":{},"source":["## 1-5. Word Embedding"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T04:47:01.763729Z","iopub.status.busy":"2024-05-09T04:47:01.763352Z","iopub.status.idle":"2024-05-09T05:01:19.461687Z","shell.execute_reply":"2024-05-09T05:01:19.457096Z","shell.execute_reply.started":"2024-05-09T04:47:01.763701Z"},"trusted":true},"outputs":[],"source":["if not os.path.exists(pretrained_embedding_dir):\n","    vectors = torchtext.vocab.GloVe()\n","    pretrained_embedding = vectors.get_vecs_by_tokens(vocab.get_itos())\n","    torch.save(pretrained_embedding, pretrained_embedding_dir)\n","else:\n","    pretrained_embedding = torch.load(pretrained_embedding_dir)"]},{"cell_type":"markdown","metadata":{},"source":["## 1-6. Prepare for Data Loading"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T06:32:36.896177Z","iopub.status.busy":"2024-05-09T06:32:36.894792Z","iopub.status.idle":"2024-05-09T06:32:36.902278Z","shell.execute_reply":"2024-05-09T06:32:36.901112Z","shell.execute_reply.started":"2024-05-09T06:32:36.896132Z"},"trusted":true},"outputs":[],"source":["class CustomDataset(Dataset):\n","    def __init__(self, data):\n","        self.data = data\n","    \n","    def __len__(self):\n","        return len(self.data)\n","    \n","    def __getitem__(self, idx):\n","        item = self.data[idx]\n","        return item"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T06:32:37.040510Z","iopub.status.busy":"2024-05-09T06:32:37.040090Z","iopub.status.idle":"2024-05-09T06:32:37.046037Z","shell.execute_reply":"2024-05-09T06:32:37.044861Z","shell.execute_reply.started":"2024-05-09T06:32:37.040475Z"},"trusted":true},"outputs":[],"source":["train_dataset = CustomDataset(train_data)\n","test_dataset = CustomDataset(test_data)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T06:32:37.192543Z","iopub.status.busy":"2024-05-09T06:32:37.191817Z","iopub.status.idle":"2024-05-09T06:32:37.198607Z","shell.execute_reply":"2024-05-09T06:32:37.197333Z","shell.execute_reply.started":"2024-05-09T06:32:37.192501Z"},"trusted":true},"outputs":[],"source":["def custom_collate_fn(batch):\n","    \n","    batch_inputs = [sample['ids'] for sample in batch]\n","    batch_labels = [sample['label'] for sample in batch]\n","    \n","    collate_inputs = pad_sequence(batch_inputs, \n","                                  padding_value = pad_index, \n","                                  batch_first = True)\n","    collate_labels = torch.tensor(batch_labels)\n","    \n","    return collate_inputs, collate_labels"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T06:32:37.329915Z","iopub.status.busy":"2024-05-09T06:32:37.328765Z","iopub.status.idle":"2024-05-09T06:32:37.336910Z","shell.execute_reply":"2024-05-09T06:32:37.335768Z","shell.execute_reply.started":"2024-05-09T06:32:37.329870Z"},"trusted":true},"outputs":[],"source":["batch_size = 128\n","pad_index = pad_index\n","shuffle = True\n","\n","trainloader = DataLoader(dataset = train_dataset,\n","                         batch_size = batch_size,\n","                         collate_fn = custom_collate_fn,\n","                         shuffle = shuffle)\n","testloader = DataLoader(dataset = test_dataset,\n","                         batch_size = batch_size,\n","                         collate_fn = custom_collate_fn,\n","                         shuffle = shuffle)"]},{"cell_type":"markdown","metadata":{},"source":["# 2. Define Model"]},{"cell_type":"markdown","metadata":{},"source":["## 2-1. Model Structure"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T06:33:20.701605Z","iopub.status.busy":"2024-05-09T06:33:20.700381Z","iopub.status.idle":"2024-05-09T06:33:20.708647Z","shell.execute_reply":"2024-05-09T06:33:20.707310Z","shell.execute_reply.started":"2024-05-09T06:33:20.701564Z"},"trusted":true},"outputs":[],"source":["class LSTMCell(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, bias = True):\n","        super().__init__()\n","        self.input_dim = input_dim\n","        self.hidden_dim = hidden_dim\n","        self.bias = bias\n","\n","        self.Wi = nn.Linear(input_dim + hidden_dim, hidden_dim, bias = bias)\n","        self.Wf = nn.Linear(input_dim + hidden_dim, hidden_dim, bias = bias)\n","        self.Wo = nn.Linear(input_dim + hidden_dim, hidden_dim, bias = bias)\n","        self.Wh = nn.Linear(input_dim + hidden_dim, hidden_dim, bias = bias)\n","        \n","    def forward(self, input, hidden):\n","        h, c = hidden\n","        concat_ih = torch.cat((input, h), 1)\n","\n","        input_gate = F.sigmoid(self.Wi(concat_ih))\n","        forget_gate = F.sigmoid(self.Wf(concat_ih))\n","        output_gate = F.sigmoid(self.Wo(concat_ih))\n","        cell_gate = F.tanh(self.Wh(concat_ih))\n","\n","        c = forget_gate * c + input_gate * cell_gate\n","        h = output_gate * F.tanh(c)\n","\n","        return h, c"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T06:33:20.862529Z","iopub.status.busy":"2024-05-09T06:33:20.862120Z","iopub.status.idle":"2024-05-09T06:33:20.873691Z","shell.execute_reply":"2024-05-09T06:33:20.872514Z","shell.execute_reply.started":"2024-05-09T06:33:20.862497Z"},"trusted":true},"outputs":[],"source":["class DeepLSTM(nn.Module):\n","    def __init__(self, input_dim, embedding_dim, hidden_dim, num_layers, output_dim, pad_index, bias = True):\n","        super().__init__()\n","        self.embedding_dim = embedding_dim\n","        self.hidden_dim = hidden_dim\n","        self.num_layers = num_layers\n","        self.output_dim = output_dim\n","        \n","        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx = pad_index)\n","        self.embedding.requires_grad_(False)\n","        \n","        self.Layers = nn.ModuleList([LSTMCell(embedding_dim, hidden_dim, bias=bias) for _ in range(num_layers)])\n","\n","        self.fc1 = nn.Linear(hidden_dim, embedding_dim)\n","        self.fc2 = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, inputs):\n","\n","        batch_size = inputs.size(0)\n","        seq_length = inputs.size(1)\n","\n","        x = self.embedding(inputs)\n","\n","        \n","        for i in range(len(self.Layers)):\n","\n","            LSTM = self.Layers[i]\n","            \n","            h0 = torch.zeros(batch_size, self.hidden_dim).to(device)\n","            c0 = torch.zeros(batch_size, self.hidden_dim).to(device)\n","            hidden = [h0, c0]\n","\n","            for t in range(seq_length):\n","\n","                input_t = x[:, t, :]\n","                h, c = LSTM.forward(input_t, hidden)\n","                hidden = [h, c]\n","                x[:, t, :] = self.fc1(h)\n","\n","        output = self.fc2(h)\n","\n","        return output"]},{"cell_type":"markdown","metadata":{},"source":["## 2-2. Hyperparameter & functions"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T06:33:21.025116Z","iopub.status.busy":"2024-05-09T06:33:21.024663Z","iopub.status.idle":"2024-05-09T06:33:21.111777Z","shell.execute_reply":"2024-05-09T06:33:21.110479Z","shell.execute_reply.started":"2024-05-09T06:33:21.025080Z"},"trusted":true},"outputs":[],"source":["vocab_size = len(vocab)\n","embedding_dim = 300\n","hidden_dim = 128\n","num_layers = 3\n","output_dim = 1\n","pad_index = pad_index\n","lr = 5e-4\n","\n","model = DeepLSTM(vocab_size, embedding_dim, hidden_dim, num_layers, output_dim, pad_index, bias = True)\n","\n","criterion = nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr)"]},{"cell_type":"markdown","metadata":{},"source":["## 2-3. Weight Initialization"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T06:33:21.345375Z","iopub.status.busy":"2024-05-09T06:33:21.344933Z","iopub.status.idle":"2024-05-09T06:33:21.352661Z","shell.execute_reply":"2024-05-09T06:33:21.351278Z","shell.execute_reply.started":"2024-05-09T06:33:21.345343Z"},"trusted":true},"outputs":[],"source":["def initialize_weights(m):\n","    if isinstance(m, nn.Linear):\n","        nn.init.xavier_normal_(m.weight)\n","        nn.init.zeros_(m.bias)\n","    elif isinstance(m, LSTMCell):\n","        for name, param in m.named_parameters():\n","            if \"bias\" in name:\n","                nn.init.zeros_(param)\n","            else:\n","                nn.init.orthogonal_(param)"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T06:33:21.522657Z","iopub.status.busy":"2024-05-09T06:33:21.522278Z","iopub.status.idle":"2024-05-09T06:33:21.532722Z","shell.execute_reply":"2024-05-09T06:33:21.531867Z","shell.execute_reply.started":"2024-05-09T06:33:21.522629Z"},"trusted":true},"outputs":[{"data":{"text/plain":["DeepLSTM(\n","  (embedding): Embedding(24897, 300, padding_idx=1)\n","  (Layers): ModuleList(\n","    (0-2): 3 x LSTMCell(\n","      (Wi): Linear(in_features=428, out_features=128, bias=True)\n","      (Wf): Linear(in_features=428, out_features=128, bias=True)\n","      (Wo): Linear(in_features=428, out_features=128, bias=True)\n","      (Wh): Linear(in_features=428, out_features=128, bias=True)\n","    )\n","  )\n","  (fc1): Linear(in_features=128, out_features=300, bias=True)\n","  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",")"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["model.apply(initialize_weights)"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T06:33:21.675452Z","iopub.status.busy":"2024-05-09T06:33:21.674714Z","iopub.status.idle":"2024-05-09T06:33:21.679695Z","shell.execute_reply":"2024-05-09T06:33:21.678909Z","shell.execute_reply.started":"2024-05-09T06:33:21.675417Z"},"trusted":true},"outputs":[],"source":["model.embedding.weight.data = pretrained_embedding"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\"Model's state_dict:\"\n","Parameter name: embedding.weight\n","    Size : torch.Size([24897, 300])\n","    Value: Parameter containing:\n","tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [ 0.2720, -0.0620, -0.1884,  ...,  0.1302, -0.1832,  0.1323],\n","        ...,\n","        [ 0.6701, -0.2717,  0.4766,  ...,  0.2786,  0.3312,  0.0230],\n","        [-0.1503,  0.5624, -0.5622,  ..., -0.4224, -0.6836,  0.0726],\n","        [ 1.1741, -0.4386,  0.3310,  ...,  0.3193, -0.2292, -0.0887]])\n","Parameter name: Layers.0.Wi.weight\n","    Size : torch.Size([128, 428])\n","    Value: Parameter containing:\n","tensor([[-0.0691, -0.0469, -0.0069,  ..., -0.0463,  0.0331,  0.0482],\n","        [-0.0132,  0.0432,  0.0836,  ...,  0.0699, -0.0441, -0.1006],\n","        [ 0.0343,  0.0028, -0.1117,  ...,  0.0473,  0.0246, -0.0328],\n","        ...,\n","        [-0.0366,  0.0080,  0.0791,  ..., -0.0270,  0.0119, -0.0892],\n","        [-0.0089,  0.0095,  0.0562,  ..., -0.0387, -0.0331, -0.0013],\n","        [-0.0275, -0.0645,  0.0564,  ...,  0.1213,  0.0495, -0.0173]],\n","       requires_grad=True)\n","Parameter name: Layers.0.Wi.bias\n","    Size : torch.Size([128])\n","    Value: Parameter containing:\n","tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n","Parameter name: Layers.0.Wf.weight\n","    Size : torch.Size([128, 428])\n","    Value: Parameter containing:\n","tensor([[ 0.0633,  0.0230,  0.0268,  ...,  0.0275,  0.1133, -0.0431],\n","        [ 0.0714,  0.0577,  0.0199,  ..., -0.0082,  0.1641,  0.0271],\n","        [ 0.0271,  0.0154, -0.0194,  ..., -0.0319, -0.0254,  0.0082],\n","        ...,\n","        [ 0.0127, -0.0036, -0.0547,  ...,  0.0756,  0.0215, -0.0570],\n","        [ 0.0215, -0.0227, -0.0480,  ...,  0.0252, -0.0333,  0.0180],\n","        [ 0.0750,  0.0237, -0.0172,  ..., -0.0112, -0.0311, -0.0601]],\n","       requires_grad=True)\n","Parameter name: Layers.0.Wf.bias\n","    Size : torch.Size([128])\n","    Value: Parameter containing:\n","tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n","Parameter name: Layers.0.Wo.weight\n","    Size : torch.Size([128, 428])\n","    Value: Parameter containing:\n","tensor([[ 0.0145,  0.0026, -0.0460,  ...,  0.0133, -0.0384,  0.0145],\n","        [ 0.0913, -0.0948, -0.0781,  ..., -0.0748,  0.0184, -0.0368],\n","        [-0.0397, -0.0425, -0.0525,  ..., -0.0028,  0.0145,  0.0306],\n","        ...,\n","        [ 0.0561,  0.0007,  0.0698,  ...,  0.0053, -0.0085,  0.0003],\n","        [-0.0007,  0.0377,  0.0097,  ...,  0.0234, -0.0028, -0.0037],\n","        [ 0.0493, -0.0322,  0.0328,  ...,  0.0256,  0.0283, -0.0155]],\n","       requires_grad=True)\n","Parameter name: Layers.0.Wo.bias\n","    Size : torch.Size([128])\n","    Value: Parameter containing:\n","tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n","Parameter name: Layers.0.Wh.weight\n","    Size : torch.Size([128, 428])\n","    Value: Parameter containing:\n","tensor([[-0.0082,  0.0839,  0.0018,  ...,  0.0791,  0.1187,  0.0823],\n","        [ 0.1425, -0.0279, -0.0799,  ..., -0.0192,  0.0127, -0.0440],\n","        [ 0.0277, -0.0917, -0.0005,  ...,  0.0256,  0.0145, -0.0447],\n","        ...,\n","        [ 0.0301,  0.0580, -0.0622,  ...,  0.0058,  0.0223,  0.0133],\n","        [ 0.0455,  0.0086,  0.0836,  ..., -0.0551,  0.0657,  0.0192],\n","        [ 0.0050,  0.0352, -0.0308,  ...,  0.0345,  0.0448,  0.0546]],\n","       requires_grad=True)\n","Parameter name: Layers.0.Wh.bias\n","    Size : torch.Size([128])\n","    Value: Parameter containing:\n","tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n","Parameter name: Layers.1.Wi.weight\n","    Size : torch.Size([128, 428])\n","    Value: Parameter containing:\n","tensor([[-0.0040,  0.0036,  0.0887,  ..., -0.0625,  0.0065,  0.0407],\n","        [-0.0482, -0.0184,  0.0763,  ...,  0.0655,  0.0206,  0.0644],\n","        [-0.0625, -0.0554, -0.0006,  ...,  0.0162, -0.0636, -0.0078],\n","        ...,\n","        [-0.0229, -0.0031,  0.0049,  ...,  0.0065,  0.0910, -0.0706],\n","        [ 0.0267,  0.0128,  0.0377,  ...,  0.0959,  0.1477, -0.0373],\n","        [ 0.0286,  0.0140,  0.0548,  ..., -0.0320, -0.1276,  0.0202]],\n","       requires_grad=True)\n","Parameter name: Layers.1.Wi.bias\n","    Size : torch.Size([128])\n","    Value: Parameter containing:\n","tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n","Parameter name: Layers.1.Wf.weight\n","    Size : torch.Size([128, 428])\n","    Value: Parameter containing:\n","tensor([[-0.0827,  0.0448,  0.0507,  ...,  0.0003, -0.0073, -0.0458],\n","        [ 0.0096, -0.0283,  0.0055,  ...,  0.0726,  0.0029,  0.0460],\n","        [-0.0292,  0.0359,  0.0753,  ...,  0.0585, -0.0210, -0.0544],\n","        ...,\n","        [-0.0548,  0.0194,  0.0239,  ...,  0.0910,  0.0002, -0.0315],\n","        [ 0.0303, -0.0537,  0.0666,  ..., -0.0224,  0.0108, -0.0349],\n","        [ 0.0063, -0.0383,  0.0294,  ..., -0.0049,  0.0180, -0.0086]],\n","       requires_grad=True)\n","Parameter name: Layers.1.Wf.bias\n","    Size : torch.Size([128])\n","    Value: Parameter containing:\n","tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n","Parameter name: Layers.1.Wo.weight\n","    Size : torch.Size([128, 428])\n","    Value: Parameter containing:\n","tensor([[-0.0090,  0.0360, -0.0170,  ...,  0.0616,  0.0232, -0.0007],\n","        [-0.0896,  0.0157, -0.0836,  ..., -0.0482, -0.0368, -0.0182],\n","        [ 0.0679,  0.0148,  0.0267,  ...,  0.0454, -0.0587, -0.0387],\n","        ...,\n","        [ 0.1128,  0.0618,  0.0343,  ...,  0.0235, -0.0307, -0.0893],\n","        [-0.0561,  0.0101, -0.0345,  ..., -0.0182, -0.0421, -0.0268],\n","        [ 0.0429,  0.0144, -0.0157,  ...,  0.0140, -0.0151, -0.0431]],\n","       requires_grad=True)\n","Parameter name: Layers.1.Wo.bias\n","    Size : torch.Size([128])\n","    Value: Parameter containing:\n","tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n","Parameter name: Layers.1.Wh.weight\n","    Size : torch.Size([128, 428])\n","    Value: Parameter containing:\n","tensor([[ 0.0073, -0.0149, -0.0514,  ...,  0.0495, -0.0468, -0.0260],\n","        [-0.0478, -0.0337, -0.0009,  ..., -0.0150,  0.0003,  0.0196],\n","        [-0.0033, -0.0111,  0.0431,  ...,  0.0007, -0.0058,  0.0334],\n","        ...,\n","        [-0.0470,  0.0046,  0.0308,  ..., -0.0533,  0.0885, -0.0142],\n","        [-0.0393,  0.0207, -0.0994,  ...,  0.0138, -0.0476,  0.0391],\n","        [-0.0814, -0.0154, -0.0632,  ...,  0.0278,  0.0039, -0.0085]],\n","       requires_grad=True)\n","Parameter name: Layers.1.Wh.bias\n","    Size : torch.Size([128])\n","    Value: Parameter containing:\n","tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n","Parameter name: Layers.2.Wi.weight\n","    Size : torch.Size([128, 428])\n","    Value: Parameter containing:\n","tensor([[-0.0037, -0.0059, -0.0054,  ...,  0.0339,  0.0319,  0.0551],\n","        [-0.0262,  0.0543, -0.0341,  ...,  0.0489,  0.0100,  0.0142],\n","        [ 0.0249, -0.0335,  0.0010,  ...,  0.0669,  0.0075,  0.0106],\n","        ...,\n","        [-0.0410, -0.0446, -0.0346,  ...,  0.1506,  0.0232, -0.0058],\n","        [-0.0049, -0.0744, -0.0136,  ...,  0.0023,  0.0404,  0.0226],\n","        [ 0.0595,  0.0297, -0.0149,  ..., -0.0119, -0.0750, -0.0101]],\n","       requires_grad=True)\n","Parameter name: Layers.2.Wi.bias\n","    Size : torch.Size([128])\n","    Value: Parameter containing:\n","tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n","Parameter name: Layers.2.Wf.weight\n","    Size : torch.Size([128, 428])\n","    Value: Parameter containing:\n","tensor([[ 0.0293, -0.0338,  0.0475,  ...,  0.0401, -0.0698,  0.0847],\n","        [ 0.0433, -0.0107, -0.0780,  ..., -0.0119,  0.0132,  0.1241],\n","        [-0.0548,  0.0623,  0.0057,  ..., -0.0732,  0.0694, -0.0491],\n","        ...,\n","        [ 0.0896, -0.0343, -0.0469,  ...,  0.1020,  0.0640, -0.0118],\n","        [ 0.0228,  0.1197, -0.0156,  ..., -0.0110,  0.0380, -0.0147],\n","        [-0.0449, -0.0301, -0.0225,  ..., -0.0547,  0.0146, -0.0304]],\n","       requires_grad=True)\n","Parameter name: Layers.2.Wf.bias\n","    Size : torch.Size([128])\n","    Value: Parameter containing:\n","tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n","Parameter name: Layers.2.Wo.weight\n","    Size : torch.Size([128, 428])\n","    Value: Parameter containing:\n","tensor([[ 0.0028,  0.0671,  0.0176,  ...,  0.0406,  0.0713,  0.0227],\n","        [ 0.0483,  0.0596,  0.0339,  ...,  0.0387, -0.0249,  0.0018],\n","        [ 0.0557, -0.0251, -0.0244,  ...,  0.0119, -0.0130,  0.0179],\n","        ...,\n","        [ 0.0149, -0.0247,  0.0705,  ..., -0.0627,  0.0234, -0.0672],\n","        [-0.0234,  0.0043,  0.0576,  ..., -0.0415, -0.0119, -0.0449],\n","        [ 0.0137,  0.0316,  0.0136,  ...,  0.0259,  0.0362,  0.0395]],\n","       requires_grad=True)\n","Parameter name: Layers.2.Wo.bias\n","    Size : torch.Size([128])\n","    Value: Parameter containing:\n","tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n","Parameter name: Layers.2.Wh.weight\n","    Size : torch.Size([128, 428])\n","    Value: Parameter containing:\n","tensor([[ 0.0601, -0.0139,  0.0399,  ..., -0.0377,  0.0419, -0.0699],\n","        [ 0.0204, -0.0176,  0.0552,  ...,  0.0036, -0.0125, -0.0339],\n","        [ 0.0244, -0.0687,  0.0292,  ...,  0.0409, -0.0571,  0.0408],\n","        ...,\n","        [-0.0679, -0.0007, -0.0285,  ..., -0.0804,  0.0277, -0.0104],\n","        [-0.0942, -0.0415, -0.1464,  ...,  0.0034, -0.0089,  0.0797],\n","        [ 0.0198, -0.0221,  0.0353,  ...,  0.0318, -0.0274, -0.1167]],\n","       requires_grad=True)\n","Parameter name: Layers.2.Wh.bias\n","    Size : torch.Size([128])\n","    Value: Parameter containing:\n","tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n","Parameter name: fc1.weight\n","    Size : torch.Size([300, 128])\n","    Value: Parameter containing:\n","tensor([[-0.0161,  0.0587, -0.0837,  ..., -0.0470, -0.0172,  0.0796],\n","        [ 0.0110, -0.0249,  0.0084,  ...,  0.1297, -0.0190,  0.0885],\n","        [ 0.1756, -0.0125, -0.0519,  ...,  0.0638, -0.0173,  0.0011],\n","        ...,\n","        [-0.0004, -0.0212,  0.0626,  ...,  0.0426,  0.1142, -0.0173],\n","        [-0.0766, -0.0288, -0.0393,  ...,  0.0116, -0.0215, -0.0479],\n","        [ 0.0408, -0.1201,  0.0436,  ...,  0.0889,  0.0076,  0.0218]],\n","       requires_grad=True)\n","Parameter name: fc1.bias\n","    Size : torch.Size([300])\n","    Value: Parameter containing:\n","tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n","Parameter name: fc2.weight\n","    Size : torch.Size([1, 128])\n","    Value: Parameter containing:\n","tensor([[ 0.0408,  0.1543, -0.1720,  0.0970, -0.1607, -0.0426,  0.1034, -0.0752,\n","         -0.0825, -0.0246,  0.2526, -0.0406,  0.2541,  0.1318, -0.0499,  0.0185,\n","         -0.1206, -0.0288,  0.1194,  0.1014,  0.0880,  0.0529,  0.0407,  0.2926,\n","          0.0968, -0.1289, -0.1363, -0.0094, -0.2107, -0.0245,  0.0851, -0.0403,\n","         -0.0030,  0.0100,  0.0642, -0.0392, -0.0013,  0.0233,  0.0058,  0.2065,\n","         -0.2990, -0.1246, -0.0396,  0.0453,  0.2952,  0.0486,  0.1262,  0.0701,\n","          0.2266,  0.1664,  0.0116,  0.0357, -0.1615, -0.1476, -0.1485,  0.0986,\n","         -0.2052,  0.0274, -0.0306, -0.0381, -0.0063,  0.1326,  0.1095, -0.1025,\n","         -0.0100, -0.0244, -0.0521,  0.1306, -0.0307, -0.2010, -0.0909,  0.1741,\n","          0.0061, -0.1067,  0.0457,  0.0227, -0.0762, -0.0359,  0.0044, -0.1651,\n","         -0.0188, -0.1026, -0.1927,  0.0156,  0.0887, -0.1773,  0.2921,  0.1483,\n","          0.2014, -0.0004, -0.0647,  0.0351, -0.0077, -0.0738,  0.2340, -0.0197,\n","          0.0900,  0.0163, -0.0220, -0.1806, -0.2695,  0.1167,  0.0387,  0.0059,\n","          0.0870, -0.0500,  0.2270, -0.2771, -0.0450,  0.0020,  0.0043, -0.1489,\n","          0.1456,  0.0296, -0.1158, -0.0353, -0.0746, -0.2674, -0.1873, -0.0379,\n","          0.0438,  0.0885,  0.2141,  0.1837, -0.0310,  0.1398, -0.0608,  0.0944]],\n","       requires_grad=True)\n","Parameter name: fc2.bias\n","    Size : torch.Size([1])\n","    Value: Parameter containing:\n","tensor([0.], requires_grad=True)\n"]}],"source":["pprint(\"Model's state_dict:\")\n","for name, param in model.named_parameters():\n","    print(f\"Parameter name: {name}\")\n","    print(f\"    Size : {param.size()}\")\n","    print(f\"    Value: {param}\")"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T06:33:21.189251Z","iopub.status.busy":"2024-05-09T06:33:21.188799Z","iopub.status.idle":"2024-05-09T06:33:21.194891Z","shell.execute_reply":"2024-05-09T06:33:21.193696Z","shell.execute_reply.started":"2024-05-09T06:33:21.189216Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The model has 697,773 trainable parameters\n"]}],"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","\n","print(f\"The model has {count_parameters(model):,} trainable parameters\")"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"data":{"text/plain":["BCEWithLogitsLoss()"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["model.to(device)\n","criterion.to(device)"]},{"cell_type":"markdown","metadata":{},"source":["## 2-4. Tensorboard"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["writer = SummaryWriter()"]},{"cell_type":"markdown","metadata":{},"source":["# 3. Train Model"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T06:33:22.006416Z","iopub.status.busy":"2024-05-09T06:33:22.005155Z","iopub.status.idle":"2024-05-09T06:33:22.014644Z","shell.execute_reply":"2024-05-09T06:33:22.013435Z","shell.execute_reply.started":"2024-05-09T06:33:22.006378Z"},"trusted":true},"outputs":[],"source":["def train_model(model, criterion, optimizer, trainloader, num_epochs):\n","    print(\"-----Training Started------\")\n","    for epoch in range(num_epochs):\n","        \n","        model.train()\n","        \n","        running_loss = 0.0\n","        \n","        for inputs, labels in tqdm(trainloader):\n","            \n","            inputs, labels = inputs.to(device), labels.to(device)\n","            \n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs.view(-1), labels.float())\n","            loss.backward()\n","            optimizer.step()\n","\n","            writer.add_scalar('Loss/train', loss.item(), epoch)\n","\n","            running_loss += loss.item() * inputs.size(0)\n","\n","        epoch_loss = running_loss / len(trainloader.dataset)\n","        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss: .4f}\")\n","\n","        torch.save(model.state_dict(), model_dir)\n","    \n","    writer.close()\n","    \n","    print(\"-----Training Completed-----\")"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-05-09T06:33:22.183260Z","iopub.status.busy":"2024-05-09T06:33:22.182381Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["-----Training Started------\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 196/196 [02:51<00:00,  1.14it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [1/16], Loss:  0.6808\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 196/196 [02:51<00:00,  1.14it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [2/16], Loss:  0.6898\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 196/196 [02:51<00:00,  1.14it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [3/16], Loss:  0.6893\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 196/196 [02:51<00:00,  1.14it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [4/16], Loss:  0.6892\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 196/196 [02:51<00:00,  1.14it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [5/16], Loss:  0.6895\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 196/196 [02:52<00:00,  1.14it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [6/16], Loss:  0.6928\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 196/196 [02:52<00:00,  1.14it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [7/16], Loss:  0.6928\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 196/196 [02:52<00:00,  1.14it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [8/16], Loss:  0.6933\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 196/196 [02:51<00:00,  1.14it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [9/16], Loss:  0.6932\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 196/196 [02:50<00:00,  1.15it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [10/16], Loss:  0.6932\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 196/196 [02:52<00:00,  1.13it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [11/16], Loss:  0.6932\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 196/196 [03:00<00:00,  1.08it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [12/16], Loss:  0.6932\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 196/196 [03:14<00:00,  1.01it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [13/16], Loss:  0.6932\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 196/196 [02:50<00:00,  1.15it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [14/16], Loss:  0.6932\n"]},{"name":"stderr","output_type":"stream","text":[" 42%|████▏     | 82/196 [01:12<01:40,  1.13it/s]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[30], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[29], line 16\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, trainloader, num_epochs)\u001b[0m\n\u001b[1;32m     14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), labels\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[0;32m---> 16\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     19\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss/train\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m.\u001b[39mitem(), epoch)\n","File \u001b[0;32m~/miniconda3/envs/study-mldl/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/study-mldl/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/study-mldl/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["num_epochs = 16\n","\n","train_model(model, criterion, optimizer, trainloader, num_epochs)"]},{"cell_type":"markdown","metadata":{},"source":["# 4. Test"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def test_model(model, testloader):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    TP = 0  # True Positives\n","    TN = 0  # True Negatives\n","    FP = 0  # False Positives\n","    FN = 0  # False Negatives\n","\n","    with torch.no_grad():\n","        for inputs, labels in testloader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            outputs = model(inputs)\n","            predictions = torch.round(F.sigmoid(outputs))\n","            total += labels.size(0)\n","\n","            predictions, labels = predictions.view(-1).cpu(), labels.cpu()\n","            \n","            correct += (predictions == labels).sum().item()\n","\n","            TP += ((predictions == 1) & (labels == 1)).sum().item()\n","            TN += ((predictions == 0) & (labels == 0)).sum().item()\n","            FP += ((predictions == 1) & (labels == 0)).sum().item()\n","            FN += ((predictions == 0) & (labels == 1)).sum().item()\n","\n","    accuracy = correct / total\n","    precision = TP / (TP + FP) if TP + FP != 0 else 0\n","    recall = TP / (TP + FN) if TP + FN != 0 else 0\n","    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n","\n","    print(f\"Accuracy on test set: {accuracy:.4f}\")\n","    print(f\"Precision on test set: {precision:.4f}\")\n","    print(f\"Recall on test set: {recall:.4f}\")\n","    print(f\"F1 Score on test set: {f1:.4f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["SimpleRNN(\n","  (embedding): Embedding(24897, 300, padding_idx=1)\n","  (Layer): RNNCell(\n","    (Wi): Linear(in_features=812, out_features=512, bias=True)\n","  )\n","  (fc): Linear(in_features=512, out_features=1, bias=True)\n",")"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["model.load_state_dict(torch.load(model_dir))\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy on test set: 0.5126\n","Precision on test set: 0.5326\n","Recall on test set: 0.2061\n","F1 Score on test set: 0.2972\n"]}],"source":["test_model(model, testloader)"]},{"cell_type":"markdown","metadata":{},"source":["# 5. Inference"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def inference(text, model, tokenizer, vocab):\n","    tokens = tokenizer(text)\n","    ids = vocab.lookup_indices(tokens)\n","    tensor = torch.LongTensor(ids).unsqueeze(dim=0).to(device)\n","\n","    output = model(tensor)\n","\n","    prediction = torch.round(F.sigmoid(output))\n","\n","    return prediction"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["SimpleRNN(\n","  (embedding): Embedding(24897, 300, padding_idx=1)\n","  (Layer): RNNCell(\n","    (Wi): Linear(in_features=812, out_features=512, bias=True)\n","  )\n","  (fc): Linear(in_features=512, out_features=1, bias=True)\n",")"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["model.load_state_dict(torch.load(model_dir))\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.0\n"]}],"source":["text = 'This film is terrible!'\n","\n","prediction = inference(text, model, tokenizer, vocab)\n","\n","print(prediction.item())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1.0\n"]}],"source":["text = 'This film is great!'\n","\n","prediction = inference(text, model, tokenizer, vocab)\n","\n","print(prediction.item())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1.0\n"]}],"source":["text = 'The best film I have ever seen!'\n","\n","prediction = inference(text, model, tokenizer, vocab)\n","\n","print(prediction.item())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.0\n"]}],"source":["text = \"This film is not terrible, it's great!\"\n","\n","prediction = inference(text, model, tokenizer, vocab)\n","\n","print(prediction.item())"]},{"cell_type":"markdown","metadata":{},"source":["# Limitation\n","\n","padding 토큰 연산 제외 / dropout / validation"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
