{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [200/469], Gen Loss: 4.1042, Dis Loss: 0.0149\n",
      "Epoch [1/100], Step [400/469], Gen Loss: 6.9325, Dis Loss: 0.0126\n",
      "Epoch [2/100], Step [200/469], Gen Loss: 1.1411, Dis Loss: 0.3796\n",
      "Epoch [2/100], Step [400/469], Gen Loss: 4.1790, Dis Loss: 0.0759\n",
      "Epoch [3/100], Step [200/469], Gen Loss: 5.4365, Dis Loss: 0.1031\n",
      "Epoch [3/100], Step [400/469], Gen Loss: 1.5140, Dis Loss: 0.9409\n",
      "Epoch [4/100], Step [200/469], Gen Loss: 2.6694, Dis Loss: 0.1984\n",
      "Epoch [4/100], Step [400/469], Gen Loss: 2.1340, Dis Loss: 0.3918\n",
      "Epoch [5/100], Step [200/469], Gen Loss: 3.4455, Dis Loss: 0.1236\n",
      "Epoch [5/100], Step [400/469], Gen Loss: 2.9834, Dis Loss: 0.1141\n",
      "Epoch [6/100], Step [200/469], Gen Loss: 3.1001, Dis Loss: 0.0640\n",
      "Epoch [6/100], Step [400/469], Gen Loss: 3.2570, Dis Loss: 0.0717\n",
      "Epoch [7/100], Step [200/469], Gen Loss: 3.0735, Dis Loss: 0.3768\n",
      "Epoch [7/100], Step [400/469], Gen Loss: 3.2917, Dis Loss: 0.0831\n",
      "Epoch [8/100], Step [200/469], Gen Loss: 3.8201, Dis Loss: 0.0829\n",
      "Epoch [8/100], Step [400/469], Gen Loss: 2.9892, Dis Loss: 0.1207\n",
      "Epoch [9/100], Step [200/469], Gen Loss: 2.5679, Dis Loss: 0.1928\n",
      "Epoch [9/100], Step [400/469], Gen Loss: 4.0136, Dis Loss: 0.2506\n",
      "Epoch [10/100], Step [200/469], Gen Loss: 2.7011, Dis Loss: 0.1132\n",
      "Epoch [10/100], Step [400/469], Gen Loss: 2.8501, Dis Loss: 0.1353\n",
      "Epoch [11/100], Step [200/469], Gen Loss: 3.6026, Dis Loss: 0.0942\n",
      "Epoch [11/100], Step [400/469], Gen Loss: 2.9077, Dis Loss: 0.1655\n",
      "Epoch [12/100], Step [200/469], Gen Loss: 1.5680, Dis Loss: 0.3237\n",
      "Epoch [12/100], Step [400/469], Gen Loss: 4.5766, Dis Loss: 0.1481\n",
      "Epoch [13/100], Step [200/469], Gen Loss: 3.1178, Dis Loss: 0.1921\n",
      "Epoch [13/100], Step [400/469], Gen Loss: 3.1660, Dis Loss: 0.1704\n",
      "Epoch [14/100], Step [200/469], Gen Loss: 2.8919, Dis Loss: 0.1965\n",
      "Epoch [14/100], Step [400/469], Gen Loss: 3.4724, Dis Loss: 0.2190\n",
      "Epoch [15/100], Step [200/469], Gen Loss: 1.7089, Dis Loss: 0.4285\n",
      "Epoch [15/100], Step [400/469], Gen Loss: 3.4143, Dis Loss: 0.2618\n",
      "Epoch [16/100], Step [200/469], Gen Loss: 3.4254, Dis Loss: 0.2314\n",
      "Epoch [16/100], Step [400/469], Gen Loss: 1.9423, Dis Loss: 0.2819\n",
      "Epoch [17/100], Step [200/469], Gen Loss: 2.5620, Dis Loss: 0.1776\n",
      "Epoch [17/100], Step [400/469], Gen Loss: 3.6018, Dis Loss: 0.1456\n",
      "Epoch [18/100], Step [200/469], Gen Loss: 3.4770, Dis Loss: 0.1940\n",
      "Epoch [18/100], Step [400/469], Gen Loss: 3.2688, Dis Loss: 0.1762\n",
      "Epoch [19/100], Step [200/469], Gen Loss: 3.9072, Dis Loss: 0.2766\n",
      "Epoch [19/100], Step [400/469], Gen Loss: 3.1396, Dis Loss: 0.1525\n",
      "Epoch [20/100], Step [200/469], Gen Loss: 3.6064, Dis Loss: 0.1632\n",
      "Epoch [20/100], Step [400/469], Gen Loss: 3.0947, Dis Loss: 0.2000\n",
      "Epoch [21/100], Step [200/469], Gen Loss: 2.9176, Dis Loss: 0.2099\n",
      "Epoch [21/100], Step [400/469], Gen Loss: 3.2489, Dis Loss: 0.1964\n",
      "Epoch [22/100], Step [200/469], Gen Loss: 3.1220, Dis Loss: 0.2278\n",
      "Epoch [22/100], Step [400/469], Gen Loss: 2.6330, Dis Loss: 0.2088\n",
      "Epoch [23/100], Step [200/469], Gen Loss: 2.2491, Dis Loss: 0.2781\n",
      "Epoch [23/100], Step [400/469], Gen Loss: 2.5297, Dis Loss: 0.2819\n",
      "Epoch [24/100], Step [200/469], Gen Loss: 3.1145, Dis Loss: 0.2939\n",
      "Epoch [24/100], Step [400/469], Gen Loss: 2.6399, Dis Loss: 0.3203\n",
      "Epoch [25/100], Step [200/469], Gen Loss: 2.7621, Dis Loss: 0.2627\n",
      "Epoch [25/100], Step [400/469], Gen Loss: 2.4373, Dis Loss: 0.2527\n",
      "Epoch [26/100], Step [200/469], Gen Loss: 2.4978, Dis Loss: 0.2546\n",
      "Epoch [26/100], Step [400/469], Gen Loss: 2.0782, Dis Loss: 0.2843\n",
      "Epoch [27/100], Step [200/469], Gen Loss: 3.4114, Dis Loss: 0.2076\n",
      "Epoch [27/100], Step [400/469], Gen Loss: 2.4881, Dis Loss: 0.2826\n",
      "Epoch [28/100], Step [200/469], Gen Loss: 3.5226, Dis Loss: 0.2057\n",
      "Epoch [28/100], Step [400/469], Gen Loss: 2.1708, Dis Loss: 0.4562\n",
      "Epoch [29/100], Step [200/469], Gen Loss: 2.8694, Dis Loss: 0.2589\n",
      "Epoch [29/100], Step [400/469], Gen Loss: 2.9380, Dis Loss: 0.2884\n",
      "Epoch [30/100], Step [200/469], Gen Loss: 2.0821, Dis Loss: 0.3156\n",
      "Epoch [30/100], Step [400/469], Gen Loss: 3.2506, Dis Loss: 0.2861\n",
      "Epoch [31/100], Step [200/469], Gen Loss: 1.9847, Dis Loss: 0.3616\n",
      "Epoch [31/100], Step [400/469], Gen Loss: 3.0071, Dis Loss: 0.2408\n",
      "Epoch [32/100], Step [200/469], Gen Loss: 2.0067, Dis Loss: 0.3621\n",
      "Epoch [32/100], Step [400/469], Gen Loss: 3.5835, Dis Loss: 0.2946\n",
      "Epoch [33/100], Step [200/469], Gen Loss: 2.8063, Dis Loss: 0.2979\n",
      "Epoch [33/100], Step [400/469], Gen Loss: 2.6765, Dis Loss: 0.2516\n",
      "Epoch [34/100], Step [200/469], Gen Loss: 2.5068, Dis Loss: 0.2468\n",
      "Epoch [34/100], Step [400/469], Gen Loss: 2.2960, Dis Loss: 0.2979\n",
      "Epoch [35/100], Step [200/469], Gen Loss: 2.2582, Dis Loss: 0.3605\n",
      "Epoch [35/100], Step [400/469], Gen Loss: 2.9162, Dis Loss: 0.2508\n",
      "Epoch [36/100], Step [200/469], Gen Loss: 1.8965, Dis Loss: 0.4256\n",
      "Epoch [36/100], Step [400/469], Gen Loss: 2.3786, Dis Loss: 0.3439\n",
      "Epoch [37/100], Step [200/469], Gen Loss: 2.7489, Dis Loss: 0.3294\n",
      "Epoch [37/100], Step [400/469], Gen Loss: 2.2937, Dis Loss: 0.3737\n",
      "Epoch [38/100], Step [200/469], Gen Loss: 2.7702, Dis Loss: 0.3001\n",
      "Epoch [38/100], Step [400/469], Gen Loss: 2.2542, Dis Loss: 0.3200\n",
      "Epoch [39/100], Step [200/469], Gen Loss: 2.7809, Dis Loss: 0.4250\n",
      "Epoch [39/100], Step [400/469], Gen Loss: 1.8748, Dis Loss: 0.3801\n",
      "Epoch [40/100], Step [200/469], Gen Loss: 2.1248, Dis Loss: 0.2648\n",
      "Epoch [40/100], Step [400/469], Gen Loss: 2.4419, Dis Loss: 0.2311\n",
      "Epoch [41/100], Step [200/469], Gen Loss: 2.1853, Dis Loss: 0.4481\n",
      "Epoch [41/100], Step [400/469], Gen Loss: 2.5275, Dis Loss: 0.3405\n",
      "Epoch [42/100], Step [200/469], Gen Loss: 2.4456, Dis Loss: 0.3159\n",
      "Epoch [42/100], Step [400/469], Gen Loss: 2.9402, Dis Loss: 0.4019\n",
      "Epoch [43/100], Step [200/469], Gen Loss: 1.6456, Dis Loss: 0.4438\n",
      "Epoch [43/100], Step [400/469], Gen Loss: 2.6351, Dis Loss: 0.2874\n",
      "Epoch [44/100], Step [200/469], Gen Loss: 1.9666, Dis Loss: 0.2973\n",
      "Epoch [44/100], Step [400/469], Gen Loss: 2.0264, Dis Loss: 0.3646\n",
      "Epoch [45/100], Step [200/469], Gen Loss: 1.8207, Dis Loss: 0.3731\n",
      "Epoch [45/100], Step [400/469], Gen Loss: 2.1806, Dis Loss: 0.3944\n",
      "Epoch [46/100], Step [200/469], Gen Loss: 2.1430, Dis Loss: 0.4116\n",
      "Epoch [46/100], Step [400/469], Gen Loss: 1.3735, Dis Loss: 0.4221\n",
      "Epoch [47/100], Step [200/469], Gen Loss: 2.6350, Dis Loss: 0.2921\n",
      "Epoch [47/100], Step [400/469], Gen Loss: 1.4978, Dis Loss: 0.4773\n",
      "Epoch [48/100], Step [200/469], Gen Loss: 2.0910, Dis Loss: 0.3127\n",
      "Epoch [48/100], Step [400/469], Gen Loss: 2.2595, Dis Loss: 0.4008\n",
      "Epoch [49/100], Step [200/469], Gen Loss: 2.0223, Dis Loss: 0.4084\n",
      "Epoch [49/100], Step [400/469], Gen Loss: 2.0301, Dis Loss: 0.3150\n",
      "Epoch [50/100], Step [200/469], Gen Loss: 2.3317, Dis Loss: 0.3049\n",
      "Epoch [50/100], Step [400/469], Gen Loss: 2.6941, Dis Loss: 0.2806\n",
      "Epoch [51/100], Step [200/469], Gen Loss: 2.3915, Dis Loss: 0.3188\n",
      "Epoch [51/100], Step [400/469], Gen Loss: 2.2802, Dis Loss: 0.3688\n",
      "Epoch [52/100], Step [200/469], Gen Loss: 2.1247, Dis Loss: 0.2976\n",
      "Epoch [52/100], Step [400/469], Gen Loss: 2.2861, Dis Loss: 0.3884\n",
      "Epoch [53/100], Step [200/469], Gen Loss: 2.0616, Dis Loss: 0.4340\n",
      "Epoch [53/100], Step [400/469], Gen Loss: 1.6739, Dis Loss: 0.3869\n",
      "Epoch [54/100], Step [200/469], Gen Loss: 2.6165, Dis Loss: 0.3370\n",
      "Epoch [54/100], Step [400/469], Gen Loss: 1.8992, Dis Loss: 0.3331\n",
      "Epoch [55/100], Step [200/469], Gen Loss: 3.1137, Dis Loss: 0.5458\n",
      "Epoch [55/100], Step [400/469], Gen Loss: 2.1740, Dis Loss: 0.5045\n",
      "Epoch [56/100], Step [200/469], Gen Loss: 1.6953, Dis Loss: 0.3931\n",
      "Epoch [56/100], Step [400/469], Gen Loss: 1.5785, Dis Loss: 0.3515\n",
      "Epoch [57/100], Step [200/469], Gen Loss: 1.7908, Dis Loss: 0.3972\n",
      "Epoch [57/100], Step [400/469], Gen Loss: 2.3459, Dis Loss: 0.3548\n",
      "Epoch [58/100], Step [200/469], Gen Loss: 2.4806, Dis Loss: 0.3331\n",
      "Epoch [58/100], Step [400/469], Gen Loss: 1.6666, Dis Loss: 0.4238\n",
      "Epoch [59/100], Step [200/469], Gen Loss: 2.0413, Dis Loss: 0.4029\n",
      "Epoch [59/100], Step [400/469], Gen Loss: 1.5336, Dis Loss: 0.3973\n",
      "Epoch [60/100], Step [200/469], Gen Loss: 2.0461, Dis Loss: 0.3946\n",
      "Epoch [60/100], Step [400/469], Gen Loss: 1.7666, Dis Loss: 0.4094\n",
      "Epoch [61/100], Step [200/469], Gen Loss: 1.9336, Dis Loss: 0.3348\n",
      "Epoch [61/100], Step [400/469], Gen Loss: 1.8304, Dis Loss: 0.3577\n",
      "Epoch [62/100], Step [200/469], Gen Loss: 2.1666, Dis Loss: 0.3285\n",
      "Epoch [62/100], Step [400/469], Gen Loss: 1.7419, Dis Loss: 0.4757\n",
      "Epoch [63/100], Step [200/469], Gen Loss: 1.9765, Dis Loss: 0.3461\n",
      "Epoch [63/100], Step [400/469], Gen Loss: 1.5053, Dis Loss: 0.4073\n",
      "Epoch [64/100], Step [200/469], Gen Loss: 1.9470, Dis Loss: 0.3726\n",
      "Epoch [64/100], Step [400/469], Gen Loss: 1.6882, Dis Loss: 0.4222\n",
      "Epoch [65/100], Step [200/469], Gen Loss: 2.0602, Dis Loss: 0.3254\n",
      "Epoch [65/100], Step [400/469], Gen Loss: 1.3395, Dis Loss: 0.4359\n",
      "Epoch [66/100], Step [200/469], Gen Loss: 1.6198, Dis Loss: 0.4165\n",
      "Epoch [66/100], Step [400/469], Gen Loss: 1.7518, Dis Loss: 0.4029\n",
      "Epoch [67/100], Step [200/469], Gen Loss: 1.7001, Dis Loss: 0.3795\n",
      "Epoch [67/100], Step [400/469], Gen Loss: 2.4865, Dis Loss: 0.4831\n",
      "Epoch [68/100], Step [200/469], Gen Loss: 2.0155, Dis Loss: 0.3724\n",
      "Epoch [68/100], Step [400/469], Gen Loss: 1.7763, Dis Loss: 0.3734\n",
      "Epoch [69/100], Step [200/469], Gen Loss: 2.4451, Dis Loss: 0.3316\n",
      "Epoch [69/100], Step [400/469], Gen Loss: 1.4759, Dis Loss: 0.3771\n",
      "Epoch [70/100], Step [200/469], Gen Loss: 1.6869, Dis Loss: 0.3701\n",
      "Epoch [70/100], Step [400/469], Gen Loss: 2.1464, Dis Loss: 0.4438\n",
      "Epoch [71/100], Step [200/469], Gen Loss: 1.7598, Dis Loss: 0.3798\n",
      "Epoch [71/100], Step [400/469], Gen Loss: 2.0065, Dis Loss: 0.4367\n",
      "Epoch [72/100], Step [200/469], Gen Loss: 2.1184, Dis Loss: 0.3863\n",
      "Epoch [72/100], Step [400/469], Gen Loss: 1.4381, Dis Loss: 0.4737\n",
      "Epoch [73/100], Step [200/469], Gen Loss: 1.9665, Dis Loss: 0.3000\n",
      "Epoch [73/100], Step [400/469], Gen Loss: 1.7366, Dis Loss: 0.3281\n",
      "Epoch [74/100], Step [200/469], Gen Loss: 1.3850, Dis Loss: 0.4452\n",
      "Epoch [74/100], Step [400/469], Gen Loss: 2.1428, Dis Loss: 0.3466\n",
      "Epoch [75/100], Step [200/469], Gen Loss: 2.3525, Dis Loss: 0.4739\n",
      "Epoch [75/100], Step [400/469], Gen Loss: 1.4808, Dis Loss: 0.4083\n",
      "Epoch [76/100], Step [200/469], Gen Loss: 1.7561, Dis Loss: 0.3755\n",
      "Epoch [76/100], Step [400/469], Gen Loss: 2.0554, Dis Loss: 0.4244\n",
      "Epoch [77/100], Step [200/469], Gen Loss: 2.1694, Dis Loss: 0.3714\n",
      "Epoch [77/100], Step [400/469], Gen Loss: 2.0225, Dis Loss: 0.5028\n",
      "Epoch [78/100], Step [200/469], Gen Loss: 1.7843, Dis Loss: 0.3816\n",
      "Epoch [78/100], Step [400/469], Gen Loss: 1.9710, Dis Loss: 0.3158\n",
      "Epoch [79/100], Step [200/469], Gen Loss: 2.0432, Dis Loss: 0.3927\n",
      "Epoch [79/100], Step [400/469], Gen Loss: 1.8317, Dis Loss: 0.4230\n",
      "Epoch [80/100], Step [200/469], Gen Loss: 1.6947, Dis Loss: 0.4273\n",
      "Epoch [80/100], Step [400/469], Gen Loss: 2.1587, Dis Loss: 0.3982\n",
      "Epoch [81/100], Step [200/469], Gen Loss: 1.7727, Dis Loss: 0.4454\n",
      "Epoch [81/100], Step [400/469], Gen Loss: 1.8118, Dis Loss: 0.4122\n",
      "Epoch [82/100], Step [200/469], Gen Loss: 1.8657, Dis Loss: 0.4493\n",
      "Epoch [82/100], Step [400/469], Gen Loss: 1.7606, Dis Loss: 0.3536\n",
      "Epoch [83/100], Step [200/469], Gen Loss: 2.0935, Dis Loss: 0.3431\n",
      "Epoch [83/100], Step [400/469], Gen Loss: 1.7263, Dis Loss: 0.4295\n",
      "Epoch [84/100], Step [200/469], Gen Loss: 1.8841, Dis Loss: 0.3848\n",
      "Epoch [84/100], Step [400/469], Gen Loss: 1.4298, Dis Loss: 0.4337\n",
      "Epoch [85/100], Step [200/469], Gen Loss: 2.0128, Dis Loss: 0.3950\n",
      "Epoch [85/100], Step [400/469], Gen Loss: 1.7441, Dis Loss: 0.3779\n",
      "Epoch [86/100], Step [200/469], Gen Loss: 1.7989, Dis Loss: 0.4276\n",
      "Epoch [86/100], Step [400/469], Gen Loss: 1.5959, Dis Loss: 0.4181\n",
      "Epoch [87/100], Step [200/469], Gen Loss: 2.1496, Dis Loss: 0.3797\n",
      "Epoch [87/100], Step [400/469], Gen Loss: 1.3168, Dis Loss: 0.4133\n",
      "Epoch [88/100], Step [200/469], Gen Loss: 1.6726, Dis Loss: 0.3854\n",
      "Epoch [88/100], Step [400/469], Gen Loss: 1.8506, Dis Loss: 0.4158\n",
      "Epoch [89/100], Step [200/469], Gen Loss: 1.7518, Dis Loss: 0.3473\n",
      "Epoch [89/100], Step [400/469], Gen Loss: 1.9464, Dis Loss: 0.4445\n",
      "Epoch [90/100], Step [200/469], Gen Loss: 2.4602, Dis Loss: 0.3868\n",
      "Epoch [90/100], Step [400/469], Gen Loss: 1.5561, Dis Loss: 0.4409\n",
      "Epoch [91/100], Step [200/469], Gen Loss: 1.6314, Dis Loss: 0.4172\n",
      "Epoch [91/100], Step [400/469], Gen Loss: 1.9858, Dis Loss: 0.3492\n",
      "Epoch [92/100], Step [200/469], Gen Loss: 1.4694, Dis Loss: 0.4513\n",
      "Epoch [92/100], Step [400/469], Gen Loss: 1.6266, Dis Loss: 0.4673\n",
      "Epoch [93/100], Step [200/469], Gen Loss: 2.5729, Dis Loss: 0.4047\n",
      "Epoch [93/100], Step [400/469], Gen Loss: 1.4652, Dis Loss: 0.4278\n",
      "Epoch [94/100], Step [200/469], Gen Loss: 2.1248, Dis Loss: 0.4425\n",
      "Epoch [94/100], Step [400/469], Gen Loss: 1.5148, Dis Loss: 0.3443\n",
      "Epoch [95/100], Step [200/469], Gen Loss: 1.9952, Dis Loss: 0.3669\n",
      "Epoch [95/100], Step [400/469], Gen Loss: 1.8190, Dis Loss: 0.4210\n",
      "Epoch [96/100], Step [200/469], Gen Loss: 2.4906, Dis Loss: 0.3061\n",
      "Epoch [96/100], Step [400/469], Gen Loss: 1.1726, Dis Loss: 0.4549\n",
      "Epoch [97/100], Step [200/469], Gen Loss: 1.6151, Dis Loss: 0.4133\n",
      "Epoch [97/100], Step [400/469], Gen Loss: 1.5549, Dis Loss: 0.3874\n",
      "Epoch [98/100], Step [200/469], Gen Loss: 1.8145, Dis Loss: 0.4295\n",
      "Epoch [98/100], Step [400/469], Gen Loss: 1.7769, Dis Loss: 0.4269\n",
      "Epoch [99/100], Step [200/469], Gen Loss: 1.3529, Dis Loss: 0.3902\n",
      "Epoch [99/100], Step [400/469], Gen Loss: 1.6211, Dis Loss: 0.3962\n",
      "Epoch [100/100], Step [200/469], Gen Loss: 1.6136, Dis Loss: 0.4563\n",
      "Epoch [100/100], Step [400/469], Gen Loss: 2.3032, Dis Loss: 0.3317\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGZCAYAAABmNy2oAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAASz0lEQVR4nO3cbazXdf3H8ffvXACHCxWFAoqQkA5SUEK4yBpFpomNFqVWGuCiNBvpHG1RmVebN0xv2OaSrRmrpUuxheZcXmGb4UAsCAVZnBRWHFAzmhOEwznf/y3ef48Qnc9PDgfw8di44eH7OufDAX5Pvgf81qqqqgIAIqKhrw8AwNFDFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFDgs/vrXv8Y3vvGNGDduXLS0tERLS0uMHz8+LrvsslizZk1fH++wWrlyZVx33XWxc+fOw/6+58+fH6eeeur/vO7JJ5+MBQsWxNSpU6N///5Rq9XixRdfPOzn4Z1HFHjblixZElOnTo1Vq1bFlVdeGb///e/jwQcfjKuuuiqee+65mDZtWrS1tfX1MQ+blStXxvXXX98rUeipxx57LB599NF43/veFx//+Mf77Bwcf5r6+gAc2/70pz/FFVdcEeeff34sW7Ys+vXrl983c+bM+M53vhP33ntvtLS09OEpD23Xrl0xcODAvj5GkWuuuSauvfbaiIi45ZZb4oknnujbA3HccKfA23LTTTdFY2NjLFmypFsQ3uyCCy6IUaNGdXvbmjVrYvbs2XHyySfHgAED4owzzoh77rmn2zVLly6NWq0WK1asiG9/+9sxbNiwOOWUU2LOnDmxbdu2Az7Ob37zm5g+fXoMGjQoBg8eHOeee2785S9/6XbN/PnzY/DgwbF+/fo455xzYsiQIfGZz3wmIiIeeeSR+MIXvhDvfe97Y8CAAXHaaafFZZddFq+88krur7vuuvje974XERFjx46NWq0WtVqt24tyT86x/8fX2toa/fv3j9NPPz1++ctfHuIz3V1Dg9+69A6/sqhbZ2dnrFixIj760Y/GyJEje7xbsWJFnHXWWbFz58644447Yvny5fGRj3wkLrrooli6dOkB1y9YsCCam5vjrrvuiptvvjmeeOKJuOSSS7pdc9NNN8VXv/rVmDhxYtxzzz3xq1/9Kl577bX45Cc/GRs2bOh27d69e2P27Nkxc+bMWL58eVx//fUREdHW1hbTp0+Pn/3sZ/Hwww/Hj3/841i1alV84hOfiI6OjjzLwoULIyLit7/9bTz11FPx1FNPxZQpU4rOsXTp0rj00kvj9NNPj/vuuy9+9KMfxY033hiPP/54jz+P0CsqqNP27duriKi+8pWvHPB9+/btqzo6OvJbV1dXft+ECROqM844o+ro6Oi2+fznP1+NHDmy6uzsrKqqqn7xi19UEVFdccUV3a67+eabq4io2tvbq6qqqq1bt1ZNTU3VwoULu1332muvVSNGjKguvPDCfNu8efOqiKjuvPPOQ/7Yurq6qo6OjmrLli1VRFTLly/P7/vJT35SRUT1wgsvdNv09BydnZ3VqFGjqilTpnT7vLz44otVc3NzNWbMmEOe7a3+23mgHu4U6BVTp06N5ubm/HbrrbdGRMTmzZvj+eefj4svvjgiIvbt25ffZs2aFe3t7bFp06Zu72v27Nnd/nvy5MkREbFly5aIiPjDH/4Q+/bti7lz53Z7fwMGDIgZM2Yc9OvtX/rSlw5420svvRSXX355jB49OpqamqK5uTnGjBkTEREbN278nz/mnp5j06ZNsW3btvja174WtVot92PGjPGXxvQ5f9FM3YYNGxYtLS354vxmd911V+zatSva29u7vajv2LEjIiIWLVoUixYtOuj7ffPX8CMiTjnllG7/3b9//4iI2L17d7f3OW3atIO+v7d+/X3gwIFxwgkndHtbV1dXnHPOObFt27a45pprYtKkSTFo0KDo6uqKj33sY/mxDqWn5/jXv/4VEREjRow44JoRI0b4p6X0KVGgbo2NjTFz5sx4+OGHo729vdvfK0ycODEi4oAXuGHDhkVExOLFi2POnDkHfb+tra1F59j/PpctW5Z/sj+UN//pfL9nn3021q1bF0uXLo158+bl2zdv3nzYz7E/ctu3bz/g+w72NjiSRIG3ZfHixfHQQw/F5ZdfHsuWLYvm5uZDXt/a2hrjx4+PdevWxU033XRYznDuuedGU1NTtLW1HfTLQj2xPxT770L2W7JkyQHXvvVOpfQcra2tMXLkyLj77rvj6quvzo+9ZcuWWLly5QH/UguOJFHgbTnrrLPi9ttvj4ULF8aUKVPiW9/6Vnzwgx+MhoaGaG9vj/vuuy8iotuXa5YsWRLnnXdenHvuuTF//vx4z3veE6+++mps3Lgx/vznP8e9995bdIZTTz01brjhhvjhD38Yf//73+Nzn/tcDB06NHbs2BGrV6+OQYMG5b8w+m8mTJgQ48aNi+9///tRVVWcfPLJ8cADD8QjjzxywLWTJk2KiIjbbrst5s2bF83NzdHa2trjczQ0NMSNN94YCxYsiC9+8YvxzW9+M3bu3BnXXXfdQb+kdDAvv/xy/PGPf4yIiPXr10dExEMPPRTDhw+P4cOHx4wZM0o+hfD/+vpvujk+rF27trr00kursWPHVv37968GDBhQnXbaadXcuXOrxx577IDr161bV1144YXVu971rqq5ubkaMWJENXPmzOqOO+7Ia/b/66Onn36623bFihVVRFQrVqzo9vbf/e531ac//enqhBNOqPr371+NGTOm+vKXv1w9+uijec28efOqQYMGHfTHsGHDhuqzn/1sNWTIkGro0KHVBRdcUG3durWKiOraa6/tdu3ixYurUaNGVQ0NDQecpSfnqKqq+vnPf16NHz++6tevX/WBD3yguvPOO6t58+b16F8f7f8cHOzbjBkz/uce/ptaVVVVXwUJgKOLf5IKQBIFAJIoAJBEAYAkCgAkUQAg9fh/XjvYowEAOHb05P9AcKcAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIAqamnF7a2tha/802bNhVv4K0aGxuLN52dnb1wkneGESNGFG+2b9/eCyehL7hTACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAqlVVVfXowlqtt88Cx5zm5ubiTUdHRy+cBP63nrzcu1MAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEBq6usD8M4xYcKEunYDBgwo3ixZsqR4c+aZZxZv6tHV1VXXbs+ePcWb1157rXiza9eu4s3Xv/714s2TTz5ZvKH3uVMAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAECqVVVV9ejCWq23z8Ix5LbbbivefPe73+2Fk3Aoq1evLt5MmjSpeNOvX7/izeDBg4s3ERFvvPFGXTsievJy704BgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgDJA/GOMyeeeGLxpqmpqXjz8ssvF2+6urqKNxERjY2NxZu9e/cWbzo7O4s3LS0txZt6zhYRsXPnzuJNPQ+d27x5c/Hm/e9/f/HmvPPOK95ERKxcubJ4U++vveONB+IBUEQUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQyh+PyRFR71NpX3/99eLN1q1b6/pYpep5CmlEfU/tvP3224s3q1atKt489thjxZsbbriheBMRMXny5OLNnj17ijezZs0q3vzjH/8o3vztb38r3kRETJw4sXjz7LPP1vWx3oncKQCQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIHkg3hHQ2NhYvLnkkkvq+lhtbW3Fm3oeVHfSSScVb3bv3l28iYjYt29f8WbQoEHFm6qqijf1fL5/8IMfFG8iIj71qU8Vb9asWVO8aWgo/7Pixo0bizfPP/988SYi4qKLLirePPfcc8Wben49HA/cKQCQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIHkg3hHQr1+/4s3TTz9d18d66aWXijejR4+u62MdzXbt2lW82bJlS/Hmn//8Z/Fm1KhRxZuIiHe/+93Fm9dff714U88DHOt5AOHatWuLNxERjz/+ePHmwx/+cPGm3vMd69wpAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAg1aqqqnp0Ya3W22c5bp144onFm//85z+9cBIOZfLkycWb9evXF296+FvuAPU8qO6EE04o3rS0tBRvzjzzzOJNU1N9z+Os52GR9Tzs8HjUk1977hQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYDkKamFxo0bV7z597//Xbx59dVXizfwVvX8vp0yZUrxZvXq1cWbhob6/ky6b9++4k1zc3NdH+t44ympABQRBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGA1NTXBzjWtLW1FW9Gjx5dvPFAPA6HHj7vspuzzz67F05y+EyfPr2vj3Bcc6cAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYBUq3r4xKxardbbZznimpubizcdHR3Fm0WLFhVvbrnlluINHA5dXV3Fm6P99WHo0KHFm507dx7+g/Sxnrzcu1MAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEBq6usD9KXRo0cXb1544YXizTPPPFO8aWxsLN5ERHR2dta14/jU1FT+W/xIPdxu3759de1mzJhRvDkeH27XW9wpAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAg1aqqqnp04RF6SNbR7u677y7ejBw5snizdu3a4k1ExFVXXVXXjuNTPQ+dq/dhjKXa29vr2o0aNeown+Sdoycv9+4UAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGA1NTXB+hLF198cfFm7ty5xZuTTjqpePPKK68Ubzg2tLS01LWbM2dO8eZIPfH0mWeeKd5MmzatF07C2+VOAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIAqVZVVdWjC2u13j7Lcaurq6t409Cg18er9evX17X70Ic+dJhPcnDbtm0r3owdO7Z4s3fv3uINb09PXu698gCQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIDX19QH60p49e4o3999/f/Gmh88c7Ka5ubl4ExHR0dFR1476HvpYz8MOj6StW7cWb+p5uN3R/nmg59wpAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAg1aoePq2tnoeFHUmNjY3Fm87OziOyqechdQMHDizeRHgw2X4NDeV/3qnn5/ZIqufBigMGDCje7N27t3jDsaEnv4bcKQCQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIDX19QEOl+bm5uLNG2+80QsnOdDrr79evKn34Wy7d+8u3rS1tRVvzj777OLNjh07ijcREbfeemvx5uqrr67rYx0J9f7cnnLKKcUbD7ejlDsFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAg1aqqqnp0Ya3W22c5JmzYsKF4M2zYsCOyifDzdCxobGysa9fV1XWYT8LRYvbs2cWb+++/v3jTk5d7dwoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEgeiFdo+PDhxZv29vbiza5du4o3ERFDhgypa0fET3/60+LNlVde2Qsngd7hgXgAFBEFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYDUqw/EO//884s3Dz74YPHmSGpsbCzedHZ2Fm8eeOCB4k1ExKxZs4o3/fv3L97s3bu3ePPrX/+6eBMRMXv27OLNSSedVLzp4W8FOGZ5IB4ARUQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACD16gPxADh6eCAeAEVEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQGrq6YVVVfXmOQA4CrhTACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACD9H3vhHIkMAmwXAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lr = 0.0002\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "\n",
    "# 데이터셋 로딩\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "])\n",
    "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 생성기 정의\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(100, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 784),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "# 판별기 정의\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(784, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.model(x)\n",
    "\n",
    "# 모델 초기화\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "# 손실 함수 및 최적화기 정의\n",
    "criterion = nn.BCELoss()\n",
    "gen_optimizer = optim.Adam(generator.parameters(), lr=lr)\n",
    "dis_optimizer = optim.Adam(discriminator.parameters(), lr=lr)\n",
    "\n",
    "# 학습 루프\n",
    "for epoch in range(epochs):\n",
    "    for i, (real_images, _) in enumerate(train_loader):\n",
    "        if real_images.size(0) != batch_size:  # 배치 크기가 일치하지 않는 경우 건너뜁니다\n",
    "            continue\n",
    "        real_images = real_images.to(device)\n",
    "        real_labels = torch.ones(batch_size, 1).to(device)\n",
    "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "        \n",
    "        # 생성기 학습\n",
    "        gen_optimizer.zero_grad()\n",
    "        z = torch.randn(batch_size, 100).to(device)\n",
    "        fake_images = generator(z)\n",
    "        gen_loss = criterion(discriminator(fake_images), real_labels)\n",
    "        gen_loss.backward()\n",
    "        gen_optimizer.step()\n",
    "\n",
    "        # 판별기 학습\n",
    "        dis_optimizer.zero_grad()\n",
    "        real_loss = criterion(discriminator(real_images), real_labels)\n",
    "        fake_loss = criterion(discriminator(fake_images.detach()), fake_labels)\n",
    "        dis_loss = (real_loss + fake_loss) / 2\n",
    "        dis_loss.backward()\n",
    "        dis_optimizer.step()\n",
    "\n",
    "        if (i+1) % 200 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(train_loader)}], Gen Loss: {gen_loss.item():.4f}, Dis Loss: {dis_loss.item():.4f}')\n",
    "\n",
    "# 원하는 숫자 생성\n",
    "def generate_digit(digit):\n",
    "    z = torch.randn(1, 100).to(device)\n",
    "    with torch.no_grad():\n",
    "        fake_image = generator(z)\n",
    "    fake_image = fake_image.view(28, 28).cpu().numpy()\n",
    "    plt.imshow(fake_image, cmap='gray')\n",
    "    plt.title(f'Generated {digit}')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# 생성기 테스트\n",
    "generate_digit(0)  # 숫자 0 생성\n",
    "generate_digit(1)  # 숫자 1 생성\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study-mldl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
