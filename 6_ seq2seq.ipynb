{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Set Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "import torchtext\n",
    "torchtext.disable_torchtext_deprecation_warning()\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Selected device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = './models/seq2seq_Translation_model.pth'\n",
    "pretrained_embedding_dir = './models/Glove_pretrained.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1. Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2. Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-3. Build Vocab "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-4. Numericalize Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-5. Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-6. Prepare for Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1. Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, bias = True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        \n",
    "        self.LSTM = nn.LSTM(input_size = embedding_dim, hidden_size = hidden_dim, bias = bias)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        batch_size = inputs.size(0)\n",
    "\n",
    "        x = self.embedding(inputs)\n",
    "\n",
    "        h = torch.zeros(batch_size, self.hidden_dim)\n",
    "        c = torch.zeros(batch_size, self.hidden_dim)\n",
    "\n",
    "        output, (h, c) = self.LSTM(x, (h, c))\n",
    "        \n",
    "        return h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, bias = True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        \n",
    "        self.LSTM = nn.LSTM(input_size = embedding_dim, hidden_size = hidden_dim, bias = bias)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, inputs, h, c):\n",
    "\n",
    "        batch_size = inputs.size(0)\n",
    "        seq_length = inputs.size(1)\n",
    "\n",
    "        x = self.embedding(inputs)\n",
    "\n",
    "        output, (h, c) = self.LSTM(x, (h, c))\n",
    "        \n",
    "        result = []\n",
    "        for t in range(seq_length):\n",
    "            result.append(F.sigmoid(self.fc(output[t]))) # output 클래스의 개수는 단어 집합의 수와 동일. 다중 클래스 분류 문제.\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seq2seq(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, bias = True):\n",
    "\n",
    "        self.Encoder = Encoder(input_dim, embedding_dim, hidden_dim, bias = bias)\n",
    "        self.Decoder = Decoder(input_dim, embedding_dim, hidden_dim, output_dim, bias = bias)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        h, c = self.Encoder.forward(inputs)\n",
    "        output = self.Decoder.forward(inputs, h, c)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2. Hyperparameter & functions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study-mldl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
